# Cthulu Analyzer v2.0 - Quick Reference Guide

## üöÄ Quick Start

### Run Basic Analysis
```bash
cd /home/runner/work/cthulu/cthulu/tools
python analyze_cthulu.py
```

### Run with Custom Output
```bash
python analyze_cthulu.py --output my_report.json
```

### Run on Specific Directory
```bash
python analyze_cthulu.py --root /path/to/cthulu --output report.json
```

---

## üìä Understanding the Output

### Console Summary Sections

1. **Overall Metrics** - Basic codebase statistics
   - Total files, lines, functions, classes, modules

2. **Issues Detected** - Critical and warning counts
   - Critical: Must fix immediately
   - Warnings: Should address soon

3. **ML/RL Components** - Machine learning analysis
   - Component counts and types
   - Detected architectures
   - Implementation status

4. **Top Critical Issues** - Immediate action items
   - Large files needing refactoring
   - High complexity code

5. **Code Improvement Suggestions** ‚ú® NEW
   - Total: 176 suggestions
   - By Severity: Critical, High, Medium, Low
   - By Category: Security, Performance, Maintainability, ML Best Practices

6. **Future Readiness Assessment** ‚ú® NEW
   - Overall Score: 7.4/10
   - 5 dimensional analysis
   - Specific recommendations

---

## üîç Exploring JSON Output

### Load and Explore
```python
import json

# Load the report
with open('codebase_analysis_final.json') as f:
    report = json.load(f)

# View main sections
print("Sections:", list(report.keys()))
# Output: ['summary', 'modules', 'ml_analysis', 'top_issues', 
#          'largest_files', 'most_complex_files', 'dependency_graph',
#          'recommendations', 'code_improvements', 'future_readiness']
```

### Access Code Improvements
```python
improvements = report['code_improvements']

# View summary
print(f"Total suggestions: {improvements['total_suggestions']}")

# By severity
print("Critical:", improvements['by_severity']['critical'])  # 4
print("High:", improvements['by_severity']['high'])          # 47
print("Medium:", improvements['by_severity']['medium'])      # 64
print("Low:", improvements['by_severity']['low'])            # 61

# By category
print("Security:", improvements['by_category']['security'])              # 7
print("Performance:", improvements['by_category']['performance'])        # 20
print("Maintainability:", improvements['by_category']['maintainability']) # 79
print("ML Best Practices:", improvements['by_category']['ml_best_practice']) # 70

# View critical improvements
for imp in improvements['details']['critical']:
    print(f"\nFile: {imp['file_path']}")
    print(f"Issue: {imp['issue']}")
    print(f"Fix: {imp['suggestion']}")
    print(f"Impact: {imp['impact']}")
    print(f"Effort: {imp['effort']}")
```

### Access Future Readiness
```python
readiness = report['future_readiness']

# Overall assessment
print(f"Overall Score: {readiness['overall_score']:.1f}/10")
print(f"Summary: {readiness['summary']}")

# Detailed metrics
for metric in readiness['metrics']:
    print(f"\n{metric['aspect']}: {metric['score']}/10")
    print(f"Assessment: {metric['assessment']}")
    if metric['recommendations']:
        print("Recommendations:")
        for rec in metric['recommendations']:
            print(f"  ‚Ä¢ {rec}")
```

### Access ML/RL Analysis
```python
ml = report['ml_analysis']

# Summary
print(f"ML Files: {ml['summary']['total_ml_files']}")
print(f"ML Lines: {ml['summary']['total_ml_lines']:,}")
print(f"Architectures: {ml['summary']['ml_architectures']}")

# Components by type
for comp_type, components in ml['components_by_type'].items():
    print(f"\n{comp_type}: {len(components)} components")
    for comp in components[:3]:  # Show first 3
        print(f"  ‚Ä¢ {comp['component_name']} ({comp['status']})")
```

---

## üìà Common Queries

### Find All Critical Issues
```python
critical = [
    imp for severity_list in report['code_improvements']['details'].values()
    for imp in severity_list
    if imp['severity'] == 'critical'
]
print(f"Found {len(critical)} critical issues")
```

### Find Security Vulnerabilities
```python
security_issues = [
    imp for severity_list in report['code_improvements']['details'].values()
    for imp in severity_list
    if imp['category'] == 'security'
]
print(f"Found {len(security_issues)} security issues")
for issue in security_issues:
    print(f"‚Ä¢ {issue['file_path']}: {issue['issue']}")
```

### Find ML Best Practice Violations
```python
ml_issues = [
    imp for severity_list in report['code_improvements']['details'].values()
    for imp in severity_list
    if imp['category'] == 'ml_best_practice'
]
print(f"Found {len(ml_issues)} ML best practice violations")
```

### Find Performance Issues
```python
perf_issues = [
    imp for severity_list in report['code_improvements']['details'].values()
    for imp in severity_list
    if imp['category'] == 'performance'
]
print(f"Found {len(perf_issues)} performance issues")
```

### Calculate Technical Debt by Effort
```python
from collections import defaultdict

debt_by_effort = defaultdict(list)
for severity_list in report['code_improvements']['details'].values():
    for imp in severity_list:
        debt_by_effort[imp['effort']].append(imp)

print(f"Low effort tasks: {len(debt_by_effort['low'])}")
print(f"Medium effort tasks: {len(debt_by_effort['medium'])}")
print(f"High effort tasks: {len(debt_by_effort['high'])}")
```

---

## üéØ Prioritization Guide

### Week 1 - Critical Security
```python
critical_security = [
    imp for imp in critical
    if imp['category'] == 'security'
]
print(f"Fix these {len(critical_security)} security issues first")
```

### Week 2-4 - High Priority
```python
high_priority = report['code_improvements']['details']['high'][:20]  # Top 20
print("Top 20 high-priority improvements:")
for i, imp in enumerate(high_priority, 1):
    print(f"{i}. {imp['file_path']}: {imp['issue']}")
```

### Month 2-3 - Medium Priority + Refactoring
```python
medium = report['code_improvements']['details']['medium']
large_files = report['largest_files'][:3]

print("Medium priority improvements:", len(medium))
print("\nLarge files to refactor:")
for f in large_files:
    print(f"  ‚Ä¢ {f['path']}: {f['lines']:,} lines")
```

---

## üîß Integration with Workflow

### CI/CD Integration
```bash
# Run analyzer in CI
python analyze_cthulu.py --output ci_report.json

# Check for new critical issues
python -c "
import json, sys
report = json.load(open('ci_report.json'))
critical_count = report['code_improvements']['by_severity']['critical']
if critical_count > 0:
    print(f'ERROR: {critical_count} critical issues found!')
    sys.exit(1)
"
```

### Pre-commit Hook
```bash
#!/bin/bash
# .git/hooks/pre-commit

# Run analyzer on changed files
python tools/analyze_cthulu.py --output .pre-commit-analysis.json

# Warn about new issues
python -c "
import json
report = json.load(open('.pre-commit-analysis.json'))
total = report['code_improvements']['total_suggestions']
print(f'‚ö†Ô∏è  Code Analysis: {total} improvement opportunities detected')
"
```

### Weekly Report
```bash
#!/bin/bash
# Generate weekly report

python analyze_cthulu.py --output weekly_report_$(date +%Y%m%d).json

# Send metrics to dashboard
python -c "
import json
from datetime import datetime

report = json.load(open('weekly_report_$(date +%Y%m%d).json'))

metrics = {
    'date': datetime.now().isoformat(),
    'total_lines': report['summary']['total_lines'],
    'critical_issues': report['code_improvements']['by_severity']['critical'],
    'high_issues': report['code_improvements']['by_severity']['high'],
    'readiness_score': report['future_readiness']['overall_score']
}

print(json.dumps(metrics, indent=2))
"
```

---

## üìö Additional Resources

- **Full Documentation**: `ANALYSIS_TOOLKIT_README.md`
- **ML/RL Audit**: `ML_RL_SYSTEM_AUDIT.md`
- **System Audit**: `SYSTEM_AUDIT.md`
- **System Map**: `system_map.html` (open in browser)

---

## üÜò Troubleshooting

### Issue: Analyzer runs slowly
**Solution**: The analyzer processes 288 files. This is normal and takes ~15 seconds.

### Issue: Want to skip certain directories
**Solution**: Add directories to `SKIP_DIRS` in `analyze_cthulu.py`:
```python
self.SKIP_DIRS = {
    '.git', '.venv', 'venv', '__pycache__',
    'your_custom_dir_to_skip'
}
```

### Issue: JSON file is too large
**Solution**: The JSON contains full details. Use Python to extract only what you need:
```python
import json

# Load full report
full = json.load(open('report.json'))

# Extract only summary
summary = {
    'summary': full['summary'],
    'code_improvements': full['code_improvements'],
    'future_readiness': full['future_readiness']
}

# Save compact version
with open('report_compact.json', 'w') as f:
    json.dump(summary, f, indent=2)
```

---

**Version:** 2.0  
**Last Updated:** 2026-01-17  
**Quick Guide for Enhanced Analyzer with ML/RL Deep Analysis**
